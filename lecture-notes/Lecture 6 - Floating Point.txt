Representation of fractions:
	10,1010 = 1x2^1 + 1x2^-1 + 1x2-3 = 2,625

Representation of fractions with fixed point:
	- Addition is straightforward: 01.100
								   00.100
								   ------
								   10.000

	- Multiplication:
					01.100  (like normal multiplication: 10)
					00.100 								  2
					------ 								 --
					00 000 								  0
				   000 00 								 2.
				  0110 0 								 --
				 00000 									 20
				00000
				----------
				0000110000
			=> answer: 0.11

How can we be smarter about this, to do really big numbers and really small numbers, for now we kinda locking it in; I'm handing you 6 bits, we have some contract of where the binary point is gonna be.
Or we'd have a separate VARIABLE to store the binary point position; and that could be a number that changes; that means it can slide -> "float"

So far, in the example we used a "fixed" binary point what we really want is to "float" the binary point. Why?
	Floating binary point most effective use of our limited bits (and thus more accuracy in our number representation):
		E.g., put 0.1640625 into binary. Represent as in 5-bits choosing where to put the binary point.
			... 000000.001010100000 ...
						 -----          <- we only need to send the 10101 (without 0s) plus another input indicating where the binary point is
		Store these bits ^^^^^ and keep track of the binary point 2 places to the left of the MSB
		Any other solution would lose accuracy

With floating point representation, each numeral carries an exponent field recording the whereabouts of its binary point.
The binary point can be outside the stored bits, so very large and small numbers can be represented

Scientific notation:
	In decimal:
			6 , 02 		x 		10 ^ 23
			^ ^	     			^^   ^^
			  decimal point	 		 exponent
			mantissa			radix (base)

		Normalized form: no leading 0s (exactly one digit to left of decimal point)

		Example: Alternatives to representing 1/1000000000
			Normalized: 1.0 x 10^-9
			Not normalized: 0.1 x 10^-8 or 10.0 x 10^-10

   In binary:
   		1.01 x 2^-1
   		 ^
   		 binary point

   		Computer arithmetic that supports it called floating point, because it represents numbers where the binary point is not fixed, as it is for integers.
   			Declare such variable in C as "float" (stores it in this way, in normalized form; on the left is always a 1)

Floating point representation:
	If the number is normalized then the number on the left is always 1, so why store it?
		+ 1.xxx...x 	* 	2^yyy...y
			^^^^^^^ 		  ^^^^^^^
			"significant"	  "exponent"

	Multiple of word size (32 bit)

    31  32			  23 22						 0
	| S |    Exponent   | 	   Significant		 |
	1 bit     8 bits 		     23 bits 		 ^
												 this always represents 2^-23

	S: sign (1 negative, 0 positive)
	Represent numbers as small as 1.2 x 10^-38 to as large as 3.4 x 10^38

	What if result too large? (> 3.4 x 10^38 or < -3.4 x 10^38)
		-> overflow

	What if result is too small? (> 0 and < 1.2 x 10^-38 or < 0 and > -1.2 x 10^-38)
		-> underflow

	2x10^38	   -2x10^-38    0    2x10^-38     2x10^38
	----|--------|-----|----|----|-----|--------|----
    ^^^^		-1	   ^^^^^^^^^^^	   1   		 ^^^^
	overflow			underflow				 overflow

	What would help reduce chances of overflow and/or underflow? -> more bits

IEEE 754 Floating Point Standard:
	Significant:
		- To pack more bits, leading 1 implicit for normalized numbers (remove leading 1)
		- 1 + 23 bits "single", 1 + 52 bits "double"
		- always true: 0 < significant < 1 (for normalized form)

	Note: 0 has no leading 1 -> reserve exponent value 0 just for number 0 (all 0s)
	The problem with sign magnitude -> we have two 0s (-0 and +0)
	IEEE 754 uses "biased exponent" representation.
		- Designers want FP numbers to be used even if no FP hardware; e.g. sort records with FP numbers using integer compares
		- Wanted bigger (integer) exponent field to represent bigger numbers
		- 2's complement poses a problem (because negative numbers look bigger)
		- We're going to see that the numbers are ordered EXACTLY as in sign-magnitude
			i.e. counting from binary odometer 00..00 to 11..11 goes from 0 to +MAX to -0 to -MAX to 0

	Called "Biased Notation", where bias is number subtracted to get real number
		- IEEE 754 uses bias of 127 for single precision.
		- Subtract 127 from exponent field to get exponent value

	(-1)^sign x (1 + significant) x 2^(exponent - 127)

	Double precision identical, except exponent bias of 1023

Special numbers:

	Infinity:
		Most positive exponent reserved for infinity
		Significant all zeros

	Zero:
		exponent all zeros
		significant all zeros
		sign: both cases valid

	What do I get if I calculate sqrt(-4) or 0/0:
		NaN: exponent = 255, significant nonzero
		we can make use of the significant bits to maybe define errors like division by zero, or store the line number of the error, or ...
			-> we can encode anything there

	Denorms: exponent = 0, significant nonzero


	Summary:
		Exponent	Significant 	Object
		-----------------------------------------
		0 			0 				0
		0 			nonzero 		Denorm
		1-254		anything 		+/- fp #
		255 		0 				+/- infinity
		255 		nonzero 		NaN

Examples:

	1 | 1000 0001 | 111 0000 0000 0000 0000 0000
	S 	Exponent 	Significant
		(-1)^S x (1 + significant) x 2^(exponent - 127)
		(-1)^1 x (1 + .111) x 2^(129 - 127)
		-1 x 1.111 x 2^(2)
		-111.1 in binary
		-7.5 in decimal

	1/3
		= 0.33333...
		= 0.25 + 0.0625 + 0.015625 + 0.00390625 + ...
		= 1/4 + 1/16 + 1/64 + 1/256 + ...
		= 2^(-2) + 2^(-4) + 2^(-6) + 2^(-8) + ...
		= 0.0101010101... * 2^0
		= 1.0101010101... * 2^(-2)
			sign: 0
			exponent: -2 + 127 = 125 = 0111 1101
			significant: 0101010101...

Discussion:
	Is FP add associative?
		-> no, because FP result approximates real result! (x + y) + z != x + (y + z)

	Precision & Accuracy:
		Precision: the number of bits we're throwing at the problem to represent a value (single, double, ...)
		Accuracy: is the difference between the actual value of a # and it's computer representation

		High precision permits high accuracy but doesn't guarantee it
			-> it is possible to have high precision but low accuracy

		Example: float pi = 3.14;
			pi will be represented using all 24 bits of the significant (highly precise), but is only an approximation (not accurate)

	Rounding:
		When we perform math on real numbers, we have to worry about rounding to fit the result in the significant field.
		The FP hardware carries two extra bits of precision, and then round to get the proper value
		Rounding also occurs when converting:
			- double to single precision
			- FP # to int

Always use doubles (no float)
	64 bits


